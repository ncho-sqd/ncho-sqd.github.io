<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.7.3">Jekyll</generator><link href="http://localhost:4000/atom.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2019-04-15T07:29:33-05:00</updated><id>http://localhost:4000/</id><title type="html">Curious Nacho</title><subtitle>Welcome curious person!</subtitle><author><name>{&quot;name&quot;=&gt;nil, &quot;picture&quot;=&gt;nil, &quot;email&quot;=&gt;nil, &quot;twitter&quot;=&gt;nil, &quot;links&quot;=&gt;[{&quot;title&quot;=&gt;nil, &quot;url&quot;=&gt;nil, &quot;icon&quot;=&gt;nil}]}</name></author><entry><title type="html">Predicting genres of 45,000 Project Gutenberg books using NLP - BoW Approach</title><link href="http://localhost:4000/docs/classify_gutenberg_1/" rel="alternate" type="text/html" title="Predicting genres of 45,000 Project Gutenberg books using NLP - BoW Approach" /><published>2019-04-15T00:00:00-05:00</published><updated>2019-04-15T00:00:00-05:00</updated><id>http://localhost:4000/docs/classify_gutenberg_1</id><content type="html" xml:base="http://localhost:4000/docs/classify_gutenberg_1/">&lt;p&gt;&lt;a href=&quot;https://www.gutenberg.org/wiki/Main_Page&quot;&gt;Project Gutenberg&lt;/a&gt; is a website that offers more than &lt;strong&gt;58,000 free eBooks&lt;/strong&gt;  for which U.S. copyright have expired.  It is very interesting text data for Natural Language Processing (NLP), as it is a huge body of text with pretty reliable labeling such as genre, author, publication year etc…  Here, I’ll attempt to process approximately 45,000 English books from Project Gutenberg in order to find patterns between words and the genre of the books using a &lt;a href=&quot;https://en.wikipedia.org/wiki/Bag-of-words_model&quot;&gt;Bag-of-Words (BoW)&lt;/a&gt; approach.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;: My quick and dirty approach achieves an &lt;strong&gt;~80% classification accuracy&lt;/strong&gt; on new text data.&lt;/p&gt;

&lt;script type=&quot;text/javascript&quot;&gt;window.PlotlyConfig = {MathJaxConfig: 'local'};&lt;/script&gt;
&lt;script type=&quot;text/javascript&quot;&gt;if (window.MathJax) {MathJax.Hub.Config({SVG: {font: &quot;STIX-Web&quot;}});}&lt;/script&gt;
&lt;script&gt;requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}&lt;/script&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;visualizing-45000-books-in-2d-space&quot;&gt;Visualizing ~45,000 books in 2D space&lt;/h2&gt;

&lt;p&gt;After parsing through and cleaning ~45,000 books (see bottom for details), I now have a relatively “clean” collection(bag) of words for each book.  From here, I want to see if any patterns reveal between the words used and the genre of the books.  &lt;strong&gt;Do certain words indicate that the book is of certain genre?  Is more frequent use of certain words/word combinations associated with specific genres?&lt;/strong&gt;  Below interactive chart provides some evidence that such patterns likely exist.  &lt;em&gt;Click on the legend to turn off certain series.  Double click to isolate series.  Double click again to bring all series back&lt;/em&gt;.&lt;/p&gt;
&lt;iframe width=&quot;80%&quot; height=&quot;700&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot; src=&quot;/images/2019-04-15-classify_gutenberg_1_files/tsne.html&quot;&gt;&lt;/iframe&gt;
&lt;p&gt;The chart above positons each of the ~45,000 books in 2D space based on the book’s collection of words color-coded by &lt;a href=&quot;https://www.loc.gov/catdir/cpso/lcco/&quot;&gt;Library of Congress Classification&lt;/a&gt;.  Books are written with many distinct words (certainly more than 2) so visualizing their positions on 2D space based on combinations of common/different word usage is a challenging task.  An algorithm called &lt;a href=&quot;https://lvdmaaten.github.io/tsne/&quot;&gt;t-SNE&lt;/a&gt; allows us to do this effectively by visualizing the “probabilistic” distance between all observations (books) based on word collections. Two points close together doesn’t necessarily mean that they are similar (they could though!), &lt;strong&gt;but the important takeaway is that we see a good enough non-random pattern by genere&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;The pattern becomes clearer if we turn off series &lt;strong&gt;N/A (genre not available)&lt;/strong&gt; and &lt;strong&gt;P (Language and Literature)&lt;/strong&gt;, which accounts for ~50% of the sample.  &lt;strong&gt;Some interesting observations&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Genre P (Language and Literature)&lt;/strong&gt; is most widely spread out.  This makes sense as literature is probably the most published genre with myriads of different topics, hence the spread.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Genre P (Language and Literature)&lt;/strong&gt; also has a lot of “islands” on the 3rd quadrant (lower-left), which might be revealing “topic clusters”.  Zoom-in and hover over these points to see if you can find any similarities/patterns.  I think I’ve found a cluster of Scottish and Australian literature somewhere.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Genre M (Music)&lt;/strong&gt; and &lt;strong&gt;R (Medicine)&lt;/strong&gt; have tighter clusters than other genres.  Again, we can’t say for sure that these books are necessarily similar to each other, but this might be because these two fields tend to use more jargon specific to their fields.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;With these evidence of non-random patterns, we can deduct that there is likely some structure underlying these texts, if learned correctly could inform our genre classification task.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;classifying-different-genres&quot;&gt;Classifying different genres&lt;/h1&gt;

&lt;p&gt;Let’s switch gears to classification and attempt to predict the genre of books.  It’s very unlikely that we can classify a book with reasonable accuracy if there is no underlying pattern to tell a certain genre from another.  This naturally leads to the conclusion that if we are able to come up with a model that can classify books with reasonable accuracy, then the model has learned recurring patterns underlying our large body of text.  Let’s start to see if we can formalize such pattern first by looking at all of our samples.&lt;/p&gt;

&lt;h3 id=&quot;in-sample-classification&quot;&gt;In-sample classification&lt;/h3&gt;

&lt;p&gt;As a starter, I fit a &lt;a href=&quot;https://en.wikipedia.org/wiki/Multinomial_logistic_regression&quot;&gt;multinomial logistics regression&lt;/a&gt; model on the collection of words of ~45,000 books and regress that against the Library of Congressional Classification of genres.  This model allows us to classify multiple categories (genre) based on the collection and frequency of words used in each book.&lt;/p&gt;

&lt;p&gt;In-sample classification accuracy comes in at &lt;strong&gt;~82%&lt;/strong&gt;.  This means that our model can correctly explain approximately 82% of the generes based soley on word usage.  Not bad at all.  If we unpack this overall accuracy by genere as shown in the heatmap below, we start to see that not all genre has equal performance.  Accuracy ranges from as high as 96% for genre &lt;strong&gt;P (Language and Literature)&lt;/strong&gt; to as low as 2% for &lt;strong&gt;genre V (Naval Science)&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2019-04-15-classify_gutenberg_1_files/2019-04-15-classify_gutenberg_1_18_0.png&quot; alt=&quot;png&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;If we look at &lt;strong&gt;accuracy vs. sample count per genre&lt;/strong&gt;, we can see that genres with lower sample count tend to have lower accuracy.  This makes sense as with lower sample count, there is lower variance to learn patterns from compared to other genres.  This also tells me that the overall accuracy of 80% is mostly driven by books that are represented more in the sample, which also makes sense.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2019-04-15-classify_gutenberg_1_files/2019-04-15-classify_gutenberg_1_20_0.png&quot; alt=&quot;png&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The red circled genres all have higher than 75% accuracy.  &lt;strong&gt;Genre P (Language and Literature)&lt;/strong&gt; has the highest accuracy at 96% and &lt;strong&gt;genres M(Music) and Q(Science)&lt;/strong&gt; show great accuracies of 81% and 76% respectively despite their smaller sample size.  Music genre is especially impressive and was also highlighted in the t-SNE chart above as seeming to have a strong pattern based on word usage. D is World History; B is Philosophy, Psychology and Religion; A is General Works; and Q is Science.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Genres G (Geography, Anthropology, and Recreation), U(Military Science), J(Political Science), K(Law), C(Auxiliary Sciences of History), and V(Naval Science)&lt;/strong&gt; in the blue circle all have lower than 50% accuracy.  Again, this might be due to lower sample size, or because our Bag-of-Words classification approach is “context-unaware”.  Similar set of words are used in these genres as others, but the distinguishing pattern could be in what order or in what combinations different words are used. Our current approach doesn’t pick up patterns like that.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;the-real-classification-10-fold-cross-validation-out-of-sample-accuracy&quot;&gt;The real classification: 10-fold cross validation (out-of-sample accuracy)&lt;/h3&gt;
&lt;p&gt;Let’s see how robust our result of ~80% overall acccuracy holds up in out-of-sample classification, that is &lt;strong&gt;can we predict the generes of completely new books based on other books?&lt;/strong&gt;  I’ll run this 10 times, each time using a non-overlapping 90% sample of our data to predict the genre of the other 10% as shown below.  Each time, approximately 41,000 books are used to predict the genre of 4,600 books.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;11:08:48: Testing fold 0 w/ 41204 training set and 4588 test set...
11:09:28: Testing fold 1 w/ 41207 training set and 4585 test set...
11:10:09: Testing fold 2 w/ 41208 training set and 4584 test set...
11:10:54: Testing fold 3 w/ 41210 training set and 4582 test set...
11:11:34: Testing fold 4 w/ 41212 training set and 4580 test set...
11:12:12: Testing fold 5 w/ 41214 training set and 4578 test set...
11:12:52: Testing fold 6 w/ 41216 training set and 4576 test set...
11:13:35: Testing fold 7 w/ 41217 training set and 4575 test set...
11:14:13: Testing fold 8 w/ 41219 training set and 4573 test set...
11:14:57: Testing fold 9 w/ 41221 training set and 4571 test set...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Below are charts summarizing the overall and by genre accuracy for eath of the 10 iterations.  First chart shows overall accuracy of the model where 90% of sample is fit (in-sample) and the prediction (out-of-sample classification) accuracy of when this model is applied to new data (the other 10 %) to predict the genre.  Second chart breaks down the overall out-of-sample classification accuracy by genre.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;First chart shows that the in-sample and out-of-sample accuracy is pretty close to each other.  This is good as it indicates that we are not over-fitting to sample data.  &lt;strong&gt;Consistent out-of-sample accuracy of around or over 80% seems pretty good as well&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;Second chart tells us that accuracy by genre is also pretty stable over the 10 iterations.  This is good as well as it indicates that our model is general enough and our predictions are not sensitive to different sample.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/2019-04-15-classify_gutenberg_1_files/2019-04-15-classify_gutenberg_1_26_0.png&quot; alt=&quot;png&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;Honestly, I was surprised that the classification accuracy came out pretty well (consistently around ~80%) at first take with basic text cleaning and no hyper-parameter tuning at all.  This probably indicates that there is a stronger pattern in text data relative to what I initially expected in terms of which genre it belongs to.  Identifying genre of a book might be a trivial task for human beings, as you probably know what type of book it is by reading though a few pages.  I do not know of a reasonable human performance benchmark though so we can’t say for sure that this is necessarily the case.  One thing for sure is that humans cannot read through ~45,000 books within a matter of seconds and determine the genre of them nor quantify their rationale.&lt;/p&gt;

&lt;p&gt;There are more things I can do to improve prediction accuracy.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;I can do some work around hyper-parameter tuning&lt;/li&gt;
  &lt;li&gt;Use non-linear models for classification such as Random Forest/XGBoost&lt;/li&gt;
  &lt;li&gt;Use a fundamentally different approch to constructing text data through &lt;a href=&quot;https://en.wikipedia.org/wiki/Word_embedding&quot;&gt;word embeddings&lt;/a&gt; compared to the relatively simplistic Bag-of-Words approach.  This approach would use more sophisticated models such as &lt;a href=&quot;https://en.wikipedia.org/wiki/Word2vec&quot;&gt;word2vec&lt;/a&gt; or the state-or-art &lt;a href=&quot;https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270&quot;&gt;BERT&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These are quite some work, and I may pursue a few of these when I have time in the future.  For now, I’m pretty happy with my 80% overall accuracy :).&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1 id=&quot;cleaning-and-preparing-text-data&quot;&gt;Cleaning and preparing text data&lt;/h1&gt;
&lt;p&gt;Cleaning and preparing text data was very time consuming (~2 weeks) and is a very important process in NLP that can significantly affect prediction performance.  I’ve cut some corners here and there, but below is an overview of how I prepared text data from the ~45,000 Project Gutenberg books that logistic regression is fit on.  Please read through if you’re interested in the more granular technical details on text data preparation.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Using &lt;a href=&quot;https://spacy.io/&quot;&gt;&lt;strong&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;spaCy&lt;/code&gt;&lt;/strong&gt;&lt;/a&gt;, tokenize, remove stop words and puncuations, and lemmatize.  This results in a corpus of ~9.7 million words.  There’s probably a lot of words/phrases that didn’t get parsed/cleaned correctly.  Let’s keep going on though.&lt;/li&gt;
  &lt;li&gt;Trim down corpus by removing words that occur in less than 1% and more than 70% of the books.  This results in a significantly reduced corpus of 300k words.  This is done to remove “outliers” to the extent they do not inform overall patterns. Cutoffs were determined based on how rapidly the corpus size declined with respect to these lower/higher-end cutoff parameters.  I chose 1% and 70% when the rapid decline seemed to taper off.&lt;/li&gt;
  &lt;li&gt;Construct document-term matrix using &lt;strong&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sklearn.feature_extraction.text.TfidfVectorizer&lt;/code&gt;&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;Reduce dimensionality from 300k to 300 (factor of 1,000 which is randomly selected) using &lt;strong&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;sklearn.decomposition.TruncatedSVD&lt;/code&gt;&lt;/strong&gt; by taking the left singular matrix from the &lt;a href=&quot;https://en.wikipedia.org/wiki/Singular_value_decomposition&quot;&gt;Singular Vector Decomposition (SVD)&lt;/a&gt; factors.  This is done to i) smoothen out the sparisity of the document-term-matrix and 2) reduce the dimension for accelerated computation.  Right singular matrix of SVD can also be used for &lt;a href=&quot;https://en.wikipedia.org/wiki/Latent_semantic_analysis&quot;&gt;latent semantic analysis (LSA)&lt;/a&gt;.&lt;/li&gt;
&lt;/ol&gt;</content><author><name>nacho</name></author><summary type="html">Project Gutenberg is a website that offers more than 58,000 free eBooks for which U.S. copyright have expired. It is very interesting text data for Natural Language Processing (NLP), as it is a huge body of text with pretty reliable labeling such as genre, author, publication year etc… Here, I’ll attempt to process approximately 45,000 English books from Project Gutenberg in order to find patterns between words and the genre of the books using a Bag-of-Words (BoW) approach.</summary></entry><entry><title type="html">Jupyter Notebook driven blog post</title><link href="http://localhost:4000/docs/jupyter_nb_driven_report_blog/" rel="alternate" type="text/html" title="Jupyter Notebook driven blog post" /><published>2019-01-29T00:00:00-06:00</published><updated>2019-01-29T00:00:00-06:00</updated><id>http://localhost:4000/docs/jupyter_nb_driven_report_blog</id><content type="html" xml:base="http://localhost:4000/docs/jupyter_nb_driven_report_blog/">&lt;p&gt;Jupyter notebook is a great tool for data science, and with effective use of &lt;code class=&quot;highlighter-rouge&quot;&gt;nbconvert&lt;/code&gt; it’s also great for blogging static pages such as by using &lt;a href=&quot;https://github.com/jekyll/jekyll&quot;&gt;Jekyll&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/jupyter/nbconvert&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;nbconvert&lt;/code&gt;&lt;/a&gt;, under &lt;a href=&quot;https://jupyter.org/documentation&quot;&gt;Project Jupyter&lt;/a&gt;, allows you to easily convert &lt;code class=&quot;highlighter-rouge&quot;&gt;.ipynb&lt;/code&gt; notebook files to various static formats such as markdown, HTML, PDF, LaTeX etc…&lt;/p&gt;

&lt;p&gt;For example, to convert a Jupyter notebook to markdown, it’s as simple as:&lt;/p&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;jupyter nbconvert &lt;span class=&quot;nt&quot;&gt;--to&lt;/span&gt; markdown PATH_TO_YOURFILE.ipynb
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This saves a converted &lt;strong&gt;markdown file&lt;/strong&gt; and a &lt;strong&gt;folder&lt;/strong&gt; named after the original notebook with a “_files” suffix that collects images etc.. included in the notebook.  All plots (e.g. matplotlib) embedded in the notebook get automatically saved as separate &lt;code class=&quot;highlighter-rouge&quot;&gt;.png&lt;/code&gt; images and automatically linked in the markdown file generated.&lt;/p&gt;

&lt;p&gt;In order to further leverage &lt;code class=&quot;highlighter-rouge&quot;&gt;nbconvert&lt;/code&gt; for clean and fairly automated markdown generation which can then be directly used as a blog post right away, I need to add on some additional flags and features:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;strong&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;--no-input&lt;/code&gt;&lt;/strong&gt;: Suppress print out of all code-blocks.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;--output-dir&lt;/code&gt;&lt;/strong&gt;:  Specify directory to save markdown and “_files” folder.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;--output&lt;/code&gt;&lt;/strong&gt;:  Name of markdown file to be saved in &lt;code class=&quot;highlighter-rouge&quot;&gt;--output-dir&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Move the “_files” folder to an appropriate location for compatibility with Jekyll (since Jekyll seems unable to render images saved in the same directory as where markdown is saved) using &lt;code class=&quot;highlighter-rouge&quot;&gt;rsync&lt;/code&gt; and edit markdown using &lt;code class=&quot;highlighter-rouge&quot;&gt;sed&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Below is a simple bash script that puts all of the above together:&lt;/p&gt;
&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;#!/bin/bash&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# capture input passed to bash script&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;FILE&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$1&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# extract filename and attach date prefix&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;FILENAME&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sb&quot;&gt;`&lt;/span&gt;basename &lt;span class=&quot;nv&quot;&gt;$FILE&lt;/span&gt; .ipynb&lt;span class=&quot;sb&quot;&gt;`&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;NEW_FILENAME&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;sb&quot;&gt;`&lt;/span&gt;date &lt;span class=&quot;s2&quot;&gt;&quot;+%Y-%m-%d-&quot;&lt;/span&gt;&lt;span class=&quot;sb&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;FILENAME&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# construct markdown filename and dirname including images&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;POST_NAME&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;NEW_FILENAME&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;.md&quot;&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;IMG_DIR&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;NEW_FILENAME&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;_files&quot;&lt;/span&gt;
 
&lt;span class=&quot;c&quot;&gt;# call nbconvert&lt;/span&gt;
jupyter nbconvert &lt;span class=&quot;nt&quot;&gt;--to&lt;/span&gt; markdown &lt;span class=&quot;nv&quot;&gt;$FILE&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--output-dir&lt;/span&gt; ~/git/blog/_posts &lt;span class=&quot;nt&quot;&gt;--output&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$NEW_FILENAME&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--no-input&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# rename image files in markdown to moved /images folder (to be moved)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;fp &lt;span class=&quot;k&quot;&gt;in&lt;/span&gt; ~/git/blog/_posts/&lt;span class=&quot;nv&quot;&gt;$IMG_DIR&lt;/span&gt;/&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;do
    &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;FN&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$IMG_DIR&lt;/span&gt;/&lt;span class=&quot;sb&quot;&gt;`&lt;/span&gt;basename &lt;span class=&quot;nv&quot;&gt;$fp&lt;/span&gt;&lt;span class=&quot;sb&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# change image path and center image&lt;/span&gt;
    sed &lt;span class=&quot;nt&quot;&gt;-i&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;''&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;s#(&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$FN&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;)#(/images/&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$FN&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;){: .center-image }#g&quot;&lt;/span&gt; ~/git/blog/_posts/&lt;span class=&quot;nv&quot;&gt;$POST_NAME&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;done&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# move image folder automatically created under _post to /image folder&lt;/span&gt;
rsync &lt;span class=&quot;nt&quot;&gt;-a&lt;/span&gt; ~/git/blog/_posts/&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;IMG_DIR&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt; ~/git/blog/images
rm &lt;span class=&quot;nt&quot;&gt;-rf&lt;/span&gt; ~/git/blog/_posts/&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;IMG_DIR&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I save this script as &lt;a href=&quot;https://github.com/ncho-sqd/ncho-sqd.github.io/blob/master/nbconvert.sh&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;nbconvert.sh&lt;/code&gt;&lt;/a&gt; and use:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;bash nbconvert.sh PATH_TO_YOURFILE.ipynb
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This automatically saves a converted markdown file to &lt;code class=&quot;highlighter-rouge&quot;&gt;~/git/blog/_posts&lt;/code&gt; and the embedded image files to &lt;code class=&quot;highlighter-rouge&quot;&gt;~git/blog/images&lt;/code&gt;.  This markdown file can then directly be use as a Jekyll blog post with all the right links to the image/embedded files automatically in place.&lt;/p&gt;</content><author><name>nacho</name></author><summary type="html">Jupyter notebook is a great tool for data science, and with effective use of nbconvert it’s also great for blogging static pages such as by using Jekyll.</summary></entry><entry><title type="html">How are people riding Divvy bikes in Chicago on a Monday? (Part 2)</title><link href="http://localhost:4000/docs/analyze_divvy_rss_feed_part2/" rel="alternate" type="text/html" title="How are people riding Divvy bikes in Chicago on a Monday? (Part 2)" /><published>2018-12-11T00:00:00-06:00</published><updated>2018-12-11T00:00:00-06:00</updated><id>http://localhost:4000/docs/analyze_divvy_rss_feed_part2</id><content type="html" xml:base="http://localhost:4000/docs/analyze_divvy_rss_feed_part2/">&lt;p&gt;In &lt;a href=&quot;https://ncho-sqd.github.io/2018/10/28/analyze_divvy_rss_feed_part1.html&quot;&gt;part 1&lt;/a&gt;, we looked at some basic stats for Divvy stations and how the bike counts change over time on &lt;strong&gt;Monday, October 8th, 2018&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Most notable pattern was a &lt;strong&gt;huge decrease and a sharp spike&lt;/strong&gt; in number of bikes parked during commute time (4:00-9:00pm).  A weaker yet interesting pattern was a &lt;strong&gt;pesistent decrease&lt;/strong&gt; in bikes parked from morning time leading up to 12:00pm.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2018-12-11-analyze_divvy_rss_feed_part2_files/2018-12-11-analyze_divvy_rss_feed_part2_6_0.png&quot; alt=&quot;png&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Our (yet unexamined) hypothesis for these patterns were as below:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Hypothesis 1: Decrease in bike count from morning until 12:00pm is driven by high usage patterns in eastern downtown Chicago stations.&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Hypothesis 2: Dip and spike in bike count starting from 4:00pm are caused by commuters going home from downtown Chicago to north and northwest neighborhoods.&lt;/strong&gt;&lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Here, we will attempt to prove these claims by diving into the more granular patterns in individual bike stations.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h2 id=&quot;bike-count-pattern-over-time-by-station&quot;&gt;Bike count pattern over time by station&lt;/h2&gt;

&lt;p&gt;In order to understand patterns on individual station-level, let’s start by looking at &lt;strong&gt;bike counts for all of the 588 stations over time&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Each line shows a single station.  Redder lines indicate larger stations and bluer lines smaller stations.  Orange somewhere in between.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2018-12-11-analyze_divvy_rss_feed_part2_files/2018-12-11-analyze_divvy_rss_feed_part2_9_0.png&quot; alt=&quot;png&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Too much going on here, so let’s break down this chart into smaller groups of similar station size (colors).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2018-12-11-analyze_divvy_rss_feed_part2_files/2018-12-11-analyze_divvy_rss_feed_part2_11_0.png&quot; alt=&quot;png&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2018-12-11-analyze_divvy_rss_feed_part2_files/2018-12-11-analyze_divvy_rss_feed_part2_11_1.png&quot; alt=&quot;png&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2018-12-11-analyze_divvy_rss_feed_part2_files/2018-12-11-analyze_divvy_rss_feed_part2_11_2.png&quot; alt=&quot;png&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2018-12-11-analyze_divvy_rss_feed_part2_files/2018-12-11-analyze_divvy_rss_feed_part2_11_3.png&quot; alt=&quot;png&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2018-12-11-analyze_divvy_rss_feed_part2_files/2018-12-11-analyze_divvy_rss_feed_part2_11_4.png&quot; alt=&quot;png&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2018-12-11-analyze_divvy_rss_feed_part2_files/2018-12-11-analyze_divvy_rss_feed_part2_11_5.png&quot; alt=&quot;png&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;There is still a lot going on for some charts, but we start to see some patterns here.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Huge decrease and spike in bike count during commute time is &lt;strong&gt;NOT&lt;/strong&gt; driven by the largest stations (red line).&lt;/li&gt;
  &lt;li&gt;Rather, you somewhat see a &lt;strong&gt;decreasing&lt;/strong&gt; pattern for &lt;strong&gt;stations around 30-40 docks&lt;/strong&gt; during commute time (light orange, light blue).&lt;/li&gt;
  &lt;li&gt;You can also see a strong &lt;strong&gt;increasing&lt;/strong&gt; pattern for &lt;strong&gt;stations around 20 docks&lt;/strong&gt; during commute time.&lt;/li&gt;
  &lt;li&gt;This may indicate that &lt;strong&gt;bikes are moving from 30-40 dock stations to ~20 dock stations during commute time&lt;/strong&gt;.
&lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We’ve seen that &lt;strong&gt;larger stations are mostly located in the downtown Chicago area&lt;/strong&gt;, so the fact that bikes are moving from relatively larger to smaller stations during commute time seems to suggest that &lt;strong&gt;people may indeed be using Divvy a lot to go home (surrounding neighborhoods) from their workplace (downtown Chicago area) after work&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h2 id=&quot;how-do-we-know-where-people-are-going&quot;&gt;How do we know where people are going?&lt;/h2&gt;

&lt;p&gt;Ultimately, what we are trying to understand is from where to where people are moving with Divvy. The two hypothesis we laid out are essentially a claim on this &lt;strong&gt;flow of people&lt;/strong&gt; at different times of the day.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;This flow can be &lt;strong&gt;“localized”&lt;/strong&gt;, meaning that people are moving from different locations to another different enough locations, which may or may not result in weak localized movement patterns throughout the Divvy system.  This kind of pattern, if any, is more difficult to find due to its sporadic and ephemeral nature.&lt;/li&gt;
  &lt;li&gt;The flow can also be &lt;strong&gt;“global”&lt;/strong&gt; in a sense that many people follow &lt;strong&gt;similar travel patterns that a strong movement pattern manifests in our data&lt;/strong&gt;.  &lt;strong&gt;Patterns noted in our hypotheses are likely an expression of such “global” flows which we are trying to find evidence of&lt;/strong&gt;.
&lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Let’s start out by looking at &lt;strong&gt;system-wide bike flows in Divvy&lt;/strong&gt; first.  Below chart shows “net flow” of bikes throughout Divvy, which is basically the difference in bike counts for each timestamp from the previous one. Negative values mean people docked out more bikes than docked in and vice versa for positive values.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2018-12-11-analyze_divvy_rss_feed_part2_files/2018-12-11-analyze_divvy_rss_feed_part2_14_0.png&quot; alt=&quot;png&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Time intervals relevant to our two hypothesis are shaded grey above. We see an overall negative net flow in the earlier period and a negative net flow followed by a strong positive net flow in the later shaded period.&lt;/p&gt;

&lt;p&gt;Now, this is an aggregated overall trend we see in the Divvy system.  What if we can decompose &lt;strong&gt;component patterns&lt;/strong&gt;, if any, which results in the overall pattern shown above when aggregated?  If we can identify these &lt;strong&gt;“building blocks”&lt;/strong&gt; of the overall flow pattern, wouldn’t we be able to explain &lt;strong&gt;“component pattern #1 + #2 = overall grey shaded area pattern”&lt;/strong&gt;?  We can then trace back stations driving component pattern #1 and #2 and make observations on how they are contributing to the overall pattern.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;It’s much like un-building a completed lego to understand the building blocks so that you know exactly how to put them back.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h2 id=&quot;developing-the-building-blocks-to-understand-net-bike-flow&quot;&gt;Developing the building blocks to understand net bike flow&lt;/h2&gt;

&lt;p&gt;I use a statistical technique called &lt;a href=&quot;https://en.wikipedia.org/wiki/Cluster_analysis&quot;&gt;clustering&lt;/a&gt; to identify the building blocks of overall net bike flow in Divvy.&lt;/p&gt;

&lt;p&gt;In order to identify these building blocks more reliably, I divide the overall bike flow pattern above into consecutive 15-minute intervals and try to identify 10 component patterns in each 15-minute timeframe.  Stations with similar net flow pattern will be grouped together and rolled up to 1 distinct component pattern for each 15-minute interval. All of 588 stations will always belong to one of the 10 component patterns in each 15-minute interval.&lt;/p&gt;

&lt;p&gt;If we understand which of the 10 component pattern groups each of the 588 stations belong to for a given 15-minute interval, we can then identify the stations that can explain the overall pattern we see.&lt;/p&gt;

&lt;p&gt;Below is a chart showing component patterns based on the station building blocks identified in 15 minute intervals. &lt;strong&gt;You see a good separation of varying degress of positive and negative component patterns identified, when added altogether  will result in overall pattern above.&lt;/strong&gt;  Each component pattern for a given 15 minute interval include different sets of stations with similar net flow patterns.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2018-12-11-analyze_divvy_rss_feed_part2_files/2018-12-11-analyze_divvy_rss_feed_part2_18_0.png&quot; alt=&quot;png&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Let’s try to see each of the 10 component patterns separately.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2018-12-11-analyze_divvy_rss_feed_part2_files/2018-12-11-analyze_divvy_rss_feed_part2_20_0.png&quot; alt=&quot;png&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2018-12-11-analyze_divvy_rss_feed_part2_files/2018-12-11-analyze_divvy_rss_feed_part2_20_1.png&quot; alt=&quot;png&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2018-12-11-analyze_divvy_rss_feed_part2_files/2018-12-11-analyze_divvy_rss_feed_part2_20_2.png&quot; alt=&quot;png&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2018-12-11-analyze_divvy_rss_feed_part2_files/2018-12-11-analyze_divvy_rss_feed_part2_20_3.png&quot; alt=&quot;png&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2018-12-11-analyze_divvy_rss_feed_part2_files/2018-12-11-analyze_divvy_rss_feed_part2_20_4.png&quot; alt=&quot;png&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2018-12-11-analyze_divvy_rss_feed_part2_files/2018-12-11-analyze_divvy_rss_feed_part2_20_5.png&quot; alt=&quot;png&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2018-12-11-analyze_divvy_rss_feed_part2_files/2018-12-11-analyze_divvy_rss_feed_part2_20_6.png&quot; alt=&quot;png&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2018-12-11-analyze_divvy_rss_feed_part2_files/2018-12-11-analyze_divvy_rss_feed_part2_20_7.png&quot; alt=&quot;png&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2018-12-11-analyze_divvy_rss_feed_part2_files/2018-12-11-analyze_divvy_rss_feed_part2_20_8.png&quot; alt=&quot;png&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2018-12-11-analyze_divvy_rss_feed_part2_files/2018-12-11-analyze_divvy_rss_feed_part2_20_9.png&quot; alt=&quot;png&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Separation of component patterns become more clear when looked independently.  Grey-shaded areas are time intervals relevant to our two hypotheses, with the later period further broken down into negative and positive net flow periods. This results in a total of 3 time intervals to examine.&lt;/p&gt;

&lt;p&gt;You can see that the redder component patterns contribute to positive net flow and the bluer component to the opposite.  These two component patterns together build up to the overall net flow pattern, and underlying each component pattern are varying sets of stations in 15 minute intervals.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h2 id=&quot;zooming-into-specific-timeframes&quot;&gt;Zooming into specific timeframes&lt;/h2&gt;

&lt;p&gt;We will now zoom into the three shaded time intervals in the chart above and start getting into the weeds of identifying which stations are driving the redder or bluer component patterns.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Stations driving redder patterns will be stations where bikes are docking into.  Stations driving bluer patterns will be stations being docked out of.&lt;/strong&gt;  These stations in combination can explain the overall bike flows and means that &lt;strong&gt;there is a flow of people moving from bluer to redder pattern stations&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Below is a chart that shows different levels of component patterns and the stations driving them during &lt;strong&gt;9:00-12:00pm, time period relevant to hypothesis #1&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;The “Decrease” chart sums up all the bluer (negative net flow) component patterns, with further sub-component broken down into different colors in the “Decrease Sub-Component” chart.  The “Increase” chart does that same for redder (positive net flow) component patterns.&lt;/p&gt;

&lt;p&gt;The map shows where the bluer and redder component pattern stations are located for this time period.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;We can see a flow of bikes moving from east to west of downtown Chicago, as bikes should be moving from negative net flow areas (blue) to positive net flow area (red).&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Not sure why people use more bikes in east of downtown Chicago during this time, but it might be the tourists.  I think we’ve just proven hypothesis #1 though.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2018-12-11-analyze_divvy_rss_feed_part2_files/2018-12-11-analyze_divvy_rss_feed_part2_25_0.png&quot; alt=&quot;png&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Moving onto the first part of the hypothesis #2 time interval (negative net flow), we see the same set of charts for &lt;strong&gt;4:00-5:30pm&lt;/strong&gt; below.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;At this time, we can see bikes moving from the wider downtown area to the surrounding neighborhoods, more into north and northwestern neighborhoods.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This seems like the early wave of commuters going back home to neighborhoods that are concentrated along the north and northwestern axes.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2018-12-11-analyze_divvy_rss_feed_part2_files/2018-12-11-analyze_divvy_rss_feed_part2_27_0.png&quot; alt=&quot;png&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Let’s move on to the last time interval of interest which is the positive net flow period of hypothesis #2, &lt;strong&gt;5:30-9:00pm&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Here, we see &lt;strong&gt;continuous downtown to surrounding neighborhood movement pattern, but with slower negative flow from downtown and to more stations in the surrounding neighborhoods as can be seen from a wider areas colored in red&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Seems like people are continuing their way home en masse, which seems to be heavily concentrated in the north and northwestern areas surrounding downtown Chicago.&lt;/p&gt;

&lt;p&gt;I think we’ve proven hypothesis #2 here. Q.E.D.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2018-12-11-analyze_divvy_rss_feed_part2_files/2018-12-11-analyze_divvy_rss_feed_part2_29_0.png&quot; alt=&quot;png&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Maybe some of the observations above were obvious, but it surely was interesting for me to verify.&lt;/p&gt;

&lt;p&gt;Moreover, I was able to develop a process in which I can de-compose an overall bike flow pattern into smaller bits and attribute them to individual stations.  This means that &lt;strong&gt;this process could be used to potentially reveal more interesting “localized” patterns for those curious-minded&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;I’ll drop it here though :)  Thanks a lot for reading!&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h2 id=&quot;more&quot;&gt;More&lt;/h2&gt;

&lt;p&gt;Python script to download json feed from Divvy, click &lt;a href=&quot;https://github.com/lacolombe-nowifi/etl/blob/dev/scrape_divvy_rss_feed.py&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Jupyter notebook for analysis in this post, click &lt;a href=&quot;https://github.com/ncho-sqd/ncho-sqd.github.io/blob/master/original_posts/analyze_divvy_rss_feed_part2.ipynb&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;</content><author><name>nacho</name></author><summary type="html">In part 1, we looked at some basic stats for Divvy stations and how the bike counts change over time on Monday, October 8th, 2018.</summary></entry><entry><title type="html">How are people riding Divvy bikes in Chicago on a Monday? (Part 1)</title><link href="http://localhost:4000/docs/analyze_divvy_rss_feed_part1/" rel="alternate" type="text/html" title="How are people riding Divvy bikes in Chicago on a Monday? (Part 1)" /><published>2018-10-28T00:00:00-05:00</published><updated>2018-10-28T00:00:00-05:00</updated><id>http://localhost:4000/docs/analyze_divvy_rss_feed_part1</id><content type="html" xml:base="http://localhost:4000/docs/analyze_divvy_rss_feed_part1/">&lt;p&gt;&lt;a href=&quot;https://www.divvybikes.com/&quot;&gt;Divvy&lt;/a&gt; is Chicago’s bike share program which works the same way as NYC’s &lt;a href=&quot;https://www.citibikenyc.com/&quot;&gt;Citi Bike&lt;/a&gt; and Washington DC’s &lt;a href=&quot;https://www.capitalbikeshare.com/&quot;&gt;Capital Bikeshare&lt;/a&gt;.  You undock a bike from a station, ride it for 30 minutes until you dock it back into any station, with a fee.&lt;/p&gt;

&lt;p&gt;It’s a great way to get from point A to B, but I usually ride them during summertime along Lake Michigan, which is absolutely &lt;strong&gt;&lt;em&gt;GLORIOUS&lt;/em&gt;&lt;/strong&gt;.  Below is a picture I took when me and my friends went on for a ride.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2018-10-28-analyze_divvy_rss_feed_part1_files/2018-10-28-analyze_divvy_rss_feed_part1_3_0.jpeg&quot; alt=&quot;jpeg&quot; class=&quot;center-image&quot; height=&quot;600px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Divvy exposes a &lt;a href=&quot;https://feeds.divvybikes.com/stations/stations.json&quot;&gt;JSON feed&lt;/a&gt; online, which seems to update every few seconds on how many bikes are available in all the stations in real time.  This seemed like interesting data, so I collected for ~24 hours starting from Monday, October 8th, 2018 and started peaking in for any interesting patterns.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h2 id=&quot;how-large-are-the-stations-and-where-are-the-largest-stations&quot;&gt;How large are the stations, and where are the largest stations?&lt;br /&gt;&lt;/h2&gt;

&lt;p&gt;There are &lt;strong&gt;588 stations&lt;/strong&gt; that are operational as of October 8th, 2018 and the &lt;strong&gt;median dock count is 15&lt;/strong&gt;(station size).  If you’re Divvy station has more than 15 docks, now you know that you’re at a relatively larger station or not.&lt;/p&gt;

&lt;p&gt;Also, histrogram of station dock counts below shows that the majority of the stations are smaller than 20 docks.  Specifically, &lt;strong&gt;~80% (470) of stations have fewer than 20 docks&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2018-10-28-analyze_divvy_rss_feed_part1_files/2018-10-28-analyze_divvy_rss_feed_part1_8_0.png&quot; alt=&quot;png&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Moving onto the bigger stations, there are exactly 10 stations that has more than 40 docks as below.  The largest stations have 55 docks and there are 3 stations with this many docks available.&lt;/p&gt;

&lt;div&gt;
&lt;style scoped=&quot;&quot;&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt;
  &lt;thead&gt;
    &lt;tr style=&quot;text-align: right;&quot;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;station name&lt;/th&gt;
      &lt;th&gt;total docks&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;Columbus Dr &amp;amp; Randolph St&lt;/td&gt;
      &lt;td&gt;55.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;Shedd Aquarium&lt;/td&gt;
      &lt;td&gt;55.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;Field Museum&lt;/td&gt;
      &lt;td&gt;55.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;Canal St &amp;amp; Adams St&lt;/td&gt;
      &lt;td&gt;47.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;Ravenswood Ave &amp;amp; Lawrence Ave&lt;/td&gt;
      &lt;td&gt;47.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;Streeter Dr &amp;amp; Grand Ave&lt;/td&gt;
      &lt;td&gt;47.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6&lt;/th&gt;
      &lt;td&gt;Millennium Park&lt;/td&gt;
      &lt;td&gt;47.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;7&lt;/th&gt;
      &lt;td&gt;Michigan Ave &amp;amp; Washington St&lt;/td&gt;
      &lt;td&gt;43.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;8&lt;/th&gt;
      &lt;td&gt;Larrabee St &amp;amp; Kingsbury St&lt;/td&gt;
      &lt;td&gt;43.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;9&lt;/th&gt;
      &lt;td&gt;Michigan Ave &amp;amp; 8th St&lt;/td&gt;
      &lt;td&gt;42.0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;As expected, most of these stations are located in the downtown Chicago area, except for the &lt;a href=&quot;https://www.google.com/maps/place/Divvy+Station:+Ravenswood+Ave+%26+Lawrence+Ave/@41.9690647,-87.6741959,3a,75y,260.11h,90t/data=!3m7!1e1!3m5!1sNJ47KNZ-sIN2KJHE8rpdSw!2e0!6s%2F%2Fgeo3.ggpht.com%2Fcbk%3Fpanoid%3DNJ47KNZ-sIN2KJHE8rpdSw%26output%3Dthumbnail%26cb_client%3Dsearch.TACTILE.gps%26thumb%3D2%26w%3D234%26h%3D106%26yaw%3D260.11026%26pitch%3D0%26thumbfov%3D100!7i13312!8i6656!4m8!1m2!2m1!1sdivvy+Ravenswood+Ave+%26+Lawrence!3m4!1s0x880fd223bd912f4f:0x491bfb46039d36dd!8m2!3d41.9690567!4d-87.674241&quot;&gt;Ravenswood Ave &amp;amp; Lawrence&lt;/a&gt; Ave and &lt;a href=&quot;https://www.google.com/maps/place/N+Kingsbury+St+%26+N+Larrabee+St,+Chicago,+IL+60610/@41.8977745,-87.6429428,3a,75y,187.5h,90t/data=!3m7!1e1!3m5!1syF7jeG18zexd9_SrVc-dLg!2e0!6s%2F%2Fgeo3.ggpht.com%2Fcbk%3Fpanoid%3DyF7jeG18zexd9_SrVc-dLg%26output%3Dthumbnail%26cb_client%3Dsearch.TACTILE.gps%26thumb%3D2%26w%3D86%26h%3D86%26yaw%3D187.4999%26pitch%3D0%26thumbfov%3D100!7i16384!8i8192!4m5!3m4!1s0x880fd3348132faf9:0x8bc5db7cbbbbc56e!8m2!3d41.897778!4d-87.6430376&quot;&gt;Larrabee St &amp;amp; Kingsbury St&lt;/a&gt; station.&lt;/p&gt;

&lt;p&gt;The Ravenswood station features a double-sided docking design and the Larrabee station seems to be next by the Groupon HQ based on Google Street View images.  Not sure why there are huge stations there, but these might be locations with high foot-traffic outside of downtown Chicago.  Might be worth checking out once Chicago turns warmer.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h2 id=&quot;where-are-the-bikes-parked-throughout-monday&quot;&gt;Where are the bikes parked throughout Monday?&lt;/h2&gt;

&lt;p&gt;Below is an animated &lt;strong&gt;heatmap of where bikes are parked in 10 minute intervals&lt;/strong&gt;.  This might reveal some interesting travel patterns that show how people are using Divvy bikes.  Each circle is a station, where red indicates more bikes parked and ligher yellow indicates fewer bikes.  Also, larger circles indicate bigger stations with more docking spots.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2018-10-28-analyze_divvy_rss_feed_part1_files/divvy_heatmap.gif&quot; alt=&quot;gif&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;There are a few patterns we can observe:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The day starts off with a lot of bikes (red) parked in the downtown Chicago area.&lt;/li&gt;
  &lt;li&gt;The heat (red) starts to clear out a little from the eastern side of downtown Chicago since start of day.&lt;/li&gt;
  &lt;li&gt;More heat (red) clears out of downtown Chicago to north and northwest neighborhoods starting from 4:00pm.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;All in all, people seem to use Divvy (decrease in red) actively in the eastern part of downtown Chicago staring from earlier in the day, followed by a lot of bikes in downtown Chicago flushing out to north and northwest neighborhoods around commute time.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h2 id=&quot;trends-in-total-bikes-available-in-divvy&quot;&gt;Trends in total bikes available in Divvy&lt;/h2&gt;
&lt;p&gt;Below is a chart that sums up the activity observed in the heatmap above across all stations in the Divvy system.  More specifically, the total number of bikes parked in all stations are shown over time.&lt;/p&gt;

&lt;p&gt;The reason for two lines corresponding to min/max is because a station could report different numbers of bikes parked in a 1 second interval (least time measurement unit for Divvy) when there is high activity.  Grey area shows the difference of the min/max lines, which could tell us how actively bikes docked and undocked in single stations (single-station busyness).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2018-10-28-analyze_divvy_rss_feed_part1_files/2018-10-28-analyze_divvy_rss_feed_part1_21_0.png&quot; alt=&quot;png&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Consistent with what we observed from the heatmap, we see the number of bikes available decrease from start of day until  12:00pm (both the min/max lines).  This drawdown might be driven by the heat clearing out from the eastern part of downtown Chicago in the heatmap.&lt;/p&gt;

&lt;p&gt;After this comes a drastic dip in bike count from 4:00pm, followed by a spike up until 8:00pm.  I’m pretty sure that this pattern is driven by the heat flushing out of downtown Chicago and spreading to north and northwest neighborhoods in the heatmap.&lt;/p&gt;

&lt;p&gt;Single station busyness (grey area) spikes up when the number of bikes dip down to minimum shortly after 5:00pm, meaning that this is an extremely busy time, where people are not only using Divvy bikes a lot, but also docking and undocking very frequently.&lt;/p&gt;

&lt;p&gt;We’ve seen a few charts, and have a few hypothesis for the patterns that we see in them so far:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Hypothesis 1: Decrease in bike count from morning until 12:00pm is driven by high usage patterns in eastern downtown Chicago stations.&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Hypothesis 2: Dip and spike in bike count starting from 4:00pm are caused by commuters going home from downtown Chicago to north and northwest neighborhoods.&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;
In part 2, I’ll start looking into station level patterns that are driving the overall trend in the chart above to verify the hypotheses I established and dig out other interesting patterns.&lt;/p&gt;

&lt;p&gt;Hope you enjoyed reading and stay tuned to see if my guesses are correct :)&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h2 id=&quot;more&quot;&gt;More&lt;/h2&gt;

&lt;p&gt;Python script to download json feed from Divvy, click &lt;a href=&quot;https://github.com/lacolombe-nowifi/etl/blob/dev/scrape_divvy_rss_feed.py&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Jupyter notebook for analysis in this post, click &lt;a href=&quot;https://github.com/ncho-sqd/ncho-sqd.github.io/blob/master/original_posts/analyze_divvy_rss_feed_part1.ipynb&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;</content><author><name>nacho</name></author><summary type="html">Divvy is Chicago’s bike share program which works the same way as NYC’s Citi Bike and Washington DC’s Capital Bikeshare. You undock a bike from a station, ride it for 30 minutes until you dock it back into any station, with a fee.</summary></entry><entry><title type="html">How I got my Gmail inbox count down by 50% using Python</title><link href="http://localhost:4000/docs/clean_email_inbox/" rel="alternate" type="text/html" title="How I got my Gmail inbox count down by 50% using Python" /><published>2018-10-15T00:00:00-05:00</published><updated>2018-10-15T00:00:00-05:00</updated><id>http://localhost:4000/docs/clean_email_inbox</id><content type="html" xml:base="http://localhost:4000/docs/clean_email_inbox/">&lt;p&gt;My Gmail inbox was counting close to &lt;strong&gt;30,000&lt;/strong&gt;.  Not a pretty number at all, so I decided I should do something about it.&lt;/p&gt;

&lt;p&gt;Just like a good data analyst would do, I wanted to clean up my inbox in a &lt;strong&gt;&lt;em&gt;systematic and data-driven&lt;/em&gt;&lt;/strong&gt; manner.&lt;/p&gt;

&lt;p&gt;I wanted something easy, quick, and high payoff, not something like a full-blown spam classification algorithm using Natural Language Processing, although that’d be fun as well.  So I settled on a &lt;strong&gt;50% rule&lt;/strong&gt;, where I’ll get rid of at least 50% of unnecessary emails.&lt;/p&gt;

&lt;p&gt;You’d be surprise to learn that the &lt;a href=&quot;https://en.wikipedia.org/wiki/Pareto_principle&quot;&gt;pareto principle&lt;/a&gt; of 20% of causes explaining 80% of the effect also holds in my Gmail inbox.  12% of senders accounted for 80% of all emails in my case and &lt;strong&gt;only 18 senders accounted for approximately 50% of all my emails!&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2018-10-15-clean_email_inbox_files/2018-10-15-clean_email_inbox_6_0.png&quot; alt=&quot;png&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here you can see, how concentrated my inbox is with regards to unique senders.  There’s a steep curve up until the 200th unique sender, meaning that there are at least 200 email senders that contribute most to my cluttered inbox.  The red dotted horizontal line indicates 50% of my total mail count to give a sense of how quickly the curve reaches that point.&lt;/p&gt;

&lt;p&gt;Good to know, but I’m not going to review the top 200 senders because that’s already too many.  Instead, I’m going focus on the dotted square portion of the distribution for now.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2018-10-15-clean_email_inbox_files/2018-10-15-clean_email_inbox_8_0.png&quot; alt=&quot;png&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Zooming into the dotted square portion of the chart above, we can see that &lt;strong&gt;50% of all my emails (~15,000) are from the top 18 most frequent senders&lt;/strong&gt;.  Now that’s a number that I can manage to review.  Blue bars show the number of emails sent by each unique sender and the grey bars show cumulative numbers of those.&lt;/p&gt;

&lt;p&gt;I wonder who the most frequent sender is (first blue bar) as it has sent a whopping 5,000 emails to me so far.  Next up (second blue bar) is equally as persistent at around 2,000 emails.  Wonder who that is as well…&lt;/p&gt;

&lt;div&gt;
&lt;style scoped=&quot;&quot;&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt;
  &lt;thead&gt;
    &lt;tr style=&quot;text-align: left;&quot;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;name&lt;/th&gt;
      &lt;th&gt;organization&lt;/th&gt;
      &lt;th&gt;count&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;newtech-1&lt;/td&gt;
      &lt;td&gt;meetup.com&lt;/td&gt;
      &lt;td&gt;5559&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;info&lt;/td&gt;
      &lt;td&gt;meetup.com&lt;/td&gt;
      &lt;td&gt;2354&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;hello&lt;/td&gt;
      &lt;td&gt;mealpal.com&lt;/td&gt;
      &lt;td&gt;902&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;support&lt;/td&gt;
      &lt;td&gt;streeteasy.com&lt;/td&gt;
      &lt;td&gt;523&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;usmail&lt;/td&gt;
      &lt;td&gt;expediamail.com&lt;/td&gt;
      &lt;td&gt;371&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6&lt;/th&gt;
      &lt;td&gt;notify&lt;/td&gt;
      &lt;td&gt;buildinglink.com&lt;/td&gt;
      &lt;td&gt;361&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;7&lt;/th&gt;
      &lt;td&gt;travelocity&lt;/td&gt;
      &lt;td&gt;ac.travelocity.com&lt;/td&gt;
      &lt;td&gt;354&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;8&lt;/th&gt;
      &lt;td&gt;alert&lt;/td&gt;
      &lt;td&gt;indeed.com&lt;/td&gt;
      &lt;td&gt;335&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;9&lt;/th&gt;
      &lt;td&gt;Top-Free-Events-Today-announce&lt;/td&gt;
      &lt;td&gt;meetup.com&lt;/td&gt;
      &lt;td&gt;323&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;10&lt;/th&gt;
      &lt;td&gt;godiva&lt;/td&gt;
      &lt;td&gt;e.godiva.com&lt;/td&gt;
      &lt;td&gt;318&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;11&lt;/th&gt;
      &lt;td&gt;email&lt;/td&gt;
      &lt;td&gt;usa.uniqlo.com&lt;/td&gt;
      &lt;td&gt;302&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;12&lt;/th&gt;
      &lt;td&gt;XXX&lt;/td&gt;
      &lt;td&gt;uchicago.edu&lt;/td&gt;
      &lt;td&gt;299&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;13&lt;/th&gt;
      &lt;td&gt;groups-noreply&lt;/td&gt;
      &lt;td&gt;linkedin.com&lt;/td&gt;
      &lt;td&gt;288&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;14&lt;/th&gt;
      &lt;td&gt;zales&lt;/td&gt;
      &lt;td&gt;em.zales.com&lt;/td&gt;
      &lt;td&gt;287&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;15&lt;/th&gt;
      &lt;td&gt;XXX&lt;/td&gt;
      &lt;td&gt;uchicago.edu&lt;/td&gt;
      &lt;td&gt;286&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;16&lt;/th&gt;
      &lt;td&gt;katespade&lt;/td&gt;
      &lt;td&gt;em.katespade.com&lt;/td&gt;
      &lt;td&gt;274&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;17&lt;/th&gt;
      &lt;td&gt;noreply&lt;/td&gt;
      &lt;td&gt;r.groupon.com&lt;/td&gt;
      &lt;td&gt;266&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;18&lt;/th&gt;
      &lt;td&gt;info&lt;/td&gt;
      &lt;td&gt;twitter.com&lt;/td&gt;
      &lt;td&gt;240&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;Turns out, the two most frequent senders are from &lt;strong&gt;&lt;em&gt;Meetup&lt;/em&gt;&lt;/strong&gt;!  Specifically, “newtech-1”, which I remember as a New York Tech Meetup, and “info” should be some general update on all Meetup events that I subscribe to.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Mealpal&lt;/strong&gt;, which is a discounted email-based lunch subscription service that I had for a few months also sent me quite some emails, and so did &lt;strong&gt;StreetEasy, Expedia, Uniqlo, Godiva&lt;/strong&gt; etc…  Maybe I shouldn’t have given my email out to all those websites…&lt;/p&gt;

&lt;p&gt;There were some individuals from &lt;strong&gt;UChicago&lt;/strong&gt;(alma mater) as well, which I anonymized with “XXX”.  Brings back memories.&lt;/p&gt;

&lt;p&gt;I think I can delete all emails from the top 18 senders, so I extracted all mail ids corresponding to these senders and used Python and the &lt;code class=&quot;highlighter-rouge&quot;&gt;imaplib&lt;/code&gt; module to automatically delete ~13,000 emails.&lt;/p&gt;

&lt;p&gt;Voila! I just reduced my inbox count to half what is was without worrying about deleting any important emails!  Below is a progress bar that shows that I deleted 12,862 emails in 1:21 hours.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;100%|██████████| 12862/12862 [1:21:03&amp;lt;00:00,  4.23it/s]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h2 id=&quot;more&quot;&gt;More&lt;/h2&gt;
&lt;p&gt;If you’d like to see the Python script to download email data used in this process, please click &lt;a href=&quot;https://github.com/ncho-sqd/emailtools/blob/master/main.py&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;For Jupyter notebook walking through the email deletion process laid out on this page (after downloaded email data), please click &lt;a href=&quot;https://github.com/ncho-sqd/emailtools/blob/master/clean_email_inbox.ipynb&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;</content><author><name>nacho</name></author><summary type="html">My Gmail inbox was counting close to 30,000. Not a pretty number at all, so I decided I should do something about it.</summary></entry></feed>